{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaWAu5ujnB4g"
      },
      "source": [
        "# Word Embeddings and Deep Learning for NLP\n",
        "\n",
        "This notebook covers some of basic steps involved in using Deep Learning for NLP. This notebook covers:\n",
        "\n",
        "A brief overview of Word2Vec based Embeddings.\n",
        "A brief on HuggingFace Transformer :hugs: based implementation of NLP tasks\n",
        "Note: This is just an overview and not an exhaustive material on NLP with Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0h_gkXlco7E9"
      },
      "source": [
        "## Text Representation using Word2Vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8rbvEvSpKj2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "pd.options.display.max_colwidth = 200"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cna9YhwkpMX-"
      },
      "source": [
        "### Get Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFJs5ovFm-LW"
      },
      "outputs": [],
      "source": [
        "categories = ['alt.atheism', 'comp.graphics', 'sci.med']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wgexPcuFpUjs"
      },
      "outputs": [],
      "source": [
        "twenty_corpus = fetch_20newsgroups(subset='train',\n",
        "    categories=categories, shuffle=True, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mt4n_TZWpWa_"
      },
      "outputs": [],
      "source": [
        "[news.split('\\n')[1] for news in twenty_corpus.data[:10]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVhpKEv1pYX1"
      },
      "outputs": [],
      "source": [
        "twenty_corpus.target[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "swQzWs1qpa_4"
      },
      "outputs": [],
      "source": [
        "corpus = [news.split('\\n')[1] for news in twenty_corpus.data[:10]]\n",
        "labels = [categories[i] for i in twenty_corpus.target[:10]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoU7a4z0pa9C"
      },
      "outputs": [],
      "source": [
        "corpus = np.array(corpus)\n",
        "corpus_df = pd.DataFrame({'Document': corpus, \n",
        "                          'Category': labels})\n",
        "corpus_df = corpus_df[['Document', 'Category']]\n",
        "corpus_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C35Ma3AbphM2"
      },
      "source": [
        "### Preprocess Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXdZeOx7pdf0"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DInq975npddO"
      },
      "outputs": [],
      "source": [
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AGfGfxPpdaS"
      },
      "outputs": [],
      "source": [
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "norm_corpus = normalize_corpus(corpus)\n",
        "norm_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARyquvKiptSL"
      },
      "source": [
        "### Train Word2Vec Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1m_QYz8qT96"
      },
      "source": [
        "The Skip-gram model architecture usually tries to achieve the reverse of what the CBOW model does. It tries to predict the source context words (surrounding words) given a target word (the center word).\n",
        "\n",
        "Considering our simple sentence from earlier, ‚Äúthe quick brown fox jumps over the lazy dog‚Äù. If we used the CBOW model, we get pairs of (context_window, target_word) where if we consider a context window of size 2, we have examples like ([quick, fox], brown), ([the, brown], quick), ([the, dog], lazy) and so on.\n",
        "\n",
        "Now considering that the skip-gram model‚Äôs aim is to predict the context from the target word, the model typically inverts the contexts and targets, and tries to predict each context word from its target word. Hence the task becomes to predict the context [quick, fox] given target word ‚Äòbrown‚Äô or [the, brown] given target word ‚Äòquick‚Äô and so on.\n",
        "\n",
        "Thus the model tries to predict the context_window words based on the target_word.\n",
        "\n",
        "\n",
        "![skipgram_arch](../assets/skipgram_arch.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0hKGNvMopwVV"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from gensim.models import word2vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pun4KemLpwSs"
      },
      "outputs": [],
      "source": [
        "tokenized_corpus = [nltk.word_tokenize(doc) for doc in norm_corpus]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gensim\n",
        "The gensim framework, created by Radim ≈òeh≈Ø≈ôek consists of a robust, efficient and scalable implementation of the Word2Vec model. We will leverage the same on our sample toy corpus. In our workflow, we will tokenize our normalized corpus and then focus on the following four parameters in the Word2Vec model to build it.\n",
        "\n",
        "size: The word embedding dimensionality\n",
        "window: The context window size\n",
        "min_count: The minimum word count\n",
        "sample: The downsample setting for frequent words\n",
        "sg: Training model, 1 for skip-gram otherwise CBOW\n",
        "We will build a simple Word2Vec model on the corpus and visualize the embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZ2yy0YfpwQO"
      },
      "outputs": [],
      "source": [
        "# Set values for various parameters\n",
        "feature_size = 15    # Word vector dimensionality  \n",
        "window_context = 20  # Context window size                                                                                    \n",
        "min_word_count = 1   # Minimum word count                        \n",
        "sample = 1e-3        # Downsample setting for frequent words\n",
        "sg = 1               # skip-gram model\n",
        "\n",
        "w2v_model = word2vec.Word2Vec(tokenized_corpus, size=feature_size, \n",
        "                              window=window_context, min_count = min_word_count,\n",
        "                              sg=sg, sample=sample, iter=5000)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADkVnAVzp0YD"
      },
      "source": [
        "### Visualize Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39ig6TP-p3LE"
      },
      "outputs": [],
      "source": [
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "# visualize embeddings\n",
        "from sklearn.manifold import TSNE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7SkvFPZWp40x"
      },
      "outputs": [],
      "source": [
        "words = w2v_model.wv.index2word\n",
        "wvs = w2v_model.wv[words]\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42, n_iter=5000, perplexity=5)\n",
        "np.set_printoptions(suppress=True)\n",
        "T = tsne.fit_transform(wvs)\n",
        "labels = words\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.scatter(T[:, 0], T[:, 1], c='blue')\n",
        "for label, x, y in zip(labels, T[:, 0], T[:, 1]):\n",
        "    plt.annotate(label, xy=(x+1, y+1), xytext=(0, 0), textcoords='offset points')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwgNs0sSp6Xk"
      },
      "source": [
        "## Transformers ü§ó"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcQXF_tjqAdk"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ovs8dbmqD9b"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m71pFXF-qFf4"
      },
      "outputs": [],
      "source": [
        "classifier = pipeline('sentiment-analysis')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EDUhgwEnqGxH"
      },
      "outputs": [],
      "source": [
        "classifier.tokenizer('The hugging-face transformer package really simplifies NLP tasks')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8Xin8auqIMN"
      },
      "outputs": [],
      "source": [
        "classifier('The hugging-face transformer package really simplifies NLP tasks')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "04_nlp_deeplearning.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
