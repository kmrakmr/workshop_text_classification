{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "03_text_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Text Classification\n",
        "\n",
        "In this notebook, we will leverage the preprocessing and representation techniques and apply them for a text classification use-case. In this notebook, we will cover:\n",
        "\n",
        "- Apply cleanup and transform text data into a vector form\n",
        "- Work through a text classification use-case\n",
        "\n",
        "```shell\n",
        "Add image from the slide deck\n",
        "```\n",
        "\n",
        "Text classification can have a number of applications, such as:\n",
        "- Document categorization\n",
        "- Spam vs Ham\n",
        "- Review Classification\n",
        "- Fake Vs Actual News\n",
        "- Sentiment Classification and so on...\n",
        "\n"
      ],
      "metadata": {
        "id": "UqL0UBCHg7hg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Dependencies"
      ],
      "metadata": {
        "id": "gXJy38mdjZuT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nWy_qyhngCGP"
      },
      "outputs": [],
      "source": [
        "!pip install contractions\n",
        "!pip install tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Libraries"
      ],
      "metadata": {
        "id": "CHcgchhEjeET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "import unicodedata\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "bHGTr-vljgko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "KYx0yihzjgh_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Data\n",
        "We will make use of the movie review dataset for this tutorial"
      ],
      "metadata": {
        "id": "kpdYVBNgjzLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv(r'movie_reviews.csv.bz2')\n",
        "dataset.info()"
      ],
      "metadata": {
        "id": "g2QAXU-jjgfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "Xqy9dJHWj-XT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Train-Test Splits"
      ],
      "metadata": {
        "id": "8Z0dHnjmkAbq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# build train and test datasets\n",
        "reviews = dataset['review'].values\n",
        "sentiments = dataset['sentiment'].values"
      ],
      "metadata": {
        "id": "Wi4P8D5okE6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_reviews = reviews[:35000]\n",
        "train_sentiments = sentiments[:35000]"
      ],
      "metadata": {
        "id": "dNZgFhLvkE4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_reviews = reviews[35000:]\n",
        "test_sentiments = sentiments[35000:]"
      ],
      "metadata": {
        "id": "4rFcAlLYkE1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Preprocessing\n",
        "- Remove HTML/Special Characters\n",
        "- Remove accented characters\n",
        "- Lowercase"
      ],
      "metadata": {
        "id": "StrRCsxjkNZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_html_tags(text):\n",
        "  soup = BeautifulSoup(text, \"html.parser\")\n",
        "  [s.extract() for s in soup(['iframe', 'script'])]\n",
        "  stripped_text = soup.get_text()\n",
        "  stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "  return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "  return text\n",
        "\n",
        "def pre_process_corpus(docs):\n",
        "  norm_docs = []\n",
        "  for doc in tqdm(docs):\n",
        "    doc = strip_html_tags(doc)\n",
        "    doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    doc = doc.lower()\n",
        "    doc = remove_accented_chars(doc)\n",
        "    doc = contractions.fix(doc)\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z0-9\\s]', '', doc, re.I|re.A)\n",
        "    doc = re.sub(' +', ' ', doc)\n",
        "    doc = doc.strip()  \n",
        "    norm_docs.append(doc)\n",
        "  \n",
        "  return norm_docs"
      ],
      "metadata": {
        "id": "vhv0p91EkTl8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "norm_train_reviews = pre_process_corpus(train_reviews)\n",
        "norm_test_reviews = pre_process_corpus(test_reviews)"
      ],
      "metadata": {
        "id": "9ujpM3YrkiTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Engineering"
      ],
      "metadata": {
        "id": "utycIPiukpuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "KvHnLU18kszI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build BOW features on train reviews\n",
        "cv = CountVectorizer(binary=False, min_df=5, max_df=1.0, ngram_range=(1,2))\n",
        "cv_train_features = cv.fit_transform(norm_train_reviews)"
      ],
      "metadata": {
        "id": "6BMIpQWTkt-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build TFIDF features on train reviews\n",
        "tv = TfidfVectorizer(use_idf=True, min_df=5, max_df=1.0, ngram_range=(1,2),\n",
        "                     sublinear_tf=True)\n",
        "tv_train_features = tv.fit_transform(norm_train_reviews)"
      ],
      "metadata": {
        "id": "0wpJkPeskt7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "# transform test reviews into features\n",
        "cv_test_features = cv.transform(norm_test_reviews)\n",
        "tv_test_features = tv.transform(norm_test_reviews)"
      ],
      "metadata": {
        "id": "KPVPu2OUkt4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification Model: Logistic Regression\n",
        "\n",
        "Also known as the logit or logistic model, it uses the logistic (popularly also known as sigmoid) mathematical function to estimate the parameter values. These are the coefficients of all our features such that the overall loss is minimized when predicting the outcome"
      ],
      "metadata": {
        "id": "5EiZUgr-k7EG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression model on BOW features\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "q5wmaYmelEh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR with Count Vectorizer"
      ],
      "metadata": {
        "id": "CgC2ZKRKlomp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model\n",
        "lr_cv = LogisticRegression(penalty='l2', \n",
        "                        max_iter=500, \n",
        "                        C=1, \n",
        "                        solver='lbfgs', \n",
        "                        random_state=42)"
      ],
      "metadata": {
        "id": "_ZBrjG8GlIvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train with CountVectorizer Features\n",
        "# train model\n",
        "lr_cv.fit(cv_train_features, train_sentiments)"
      ],
      "metadata": {
        "id": "IXujYFTKlIkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict on test data\n",
        "lr_bow_predictions = lr_cv.predict(cv_test_features)"
      ],
      "metadata": {
        "id": "Ioq62lEtlRPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model"
      ],
      "metadata": {
        "id": "wLu9mg4nlVkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_sentiments, lr_bow_predictions))"
      ],
      "metadata": {
        "id": "-rmXkrnRlYH1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['negative', 'positive']\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, lr_bow_predictions), \n",
        "             index=labels, columns=labels)"
      ],
      "metadata": {
        "id": "RgWlqugblZrc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LR with TFIDF"
      ],
      "metadata": {
        "id": "mi3_BaRMluBw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# instantiate model\n",
        "lr_tv = LogisticRegression(penalty='l2', \n",
        "                        max_iter=500, \n",
        "                        C=1, \n",
        "                        solver='lbfgs', \n",
        "                        random_state=42)"
      ],
      "metadata": {
        "id": "5DuaWtgalvlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Train with CountVectorizer Features\n",
        "# train model\n",
        "lr_tv.fit(tv_train_features, train_sentiments)"
      ],
      "metadata": {
        "id": "tGTTCiIblyZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model"
      ],
      "metadata": {
        "id": "LAbEFoF0l7ye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_sentiments, lr_tfidf_predictions))"
      ],
      "metadata": {
        "id": "qx1ER0fll-Zy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['negative', 'positive']\n",
        "pd.DataFrame(confusion_matrix(test_sentiments, lr_tfidf_predictions), \n",
        "             index=labels, columns=labels)"
      ],
      "metadata": {
        "id": "T-yqZjmWl_Qn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}