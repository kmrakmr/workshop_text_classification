{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kmrakmr/workshop_text_classification/blob/main/notebooks/01_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu9PsVUFiLLS"
      },
      "source": [
        "# Getting Started with NLP\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/raghavbali/workshop_text_classification/blob/main/notebooks/01_getting_started.ipynb)\n",
        "\n",
        "In this notebook, we will get familiar with the world on NLP. \n",
        "Key takeaways from this notebook are:\n",
        "\n",
        "- Learn how to load a textual dataset\n",
        "- Understand the dataset using basic EDA\n",
        "- Learn how to perform basic preprocessing/cleanup to prepare the dataset\n",
        "\n",
        "![nlp_workflow.png](https://github.com/raghavbali/workshop_text_classification/blob/main/assets/nlp_workflow.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1072mTyi1VM"
      },
      "source": [
        "## Key NLP Libraries\n",
        "\n",
        "If you have been working in the Data Science/ML domain, you must have a set of _goto_ libraries and tools to do your magic. For instance, libraries like ``sklearn`` , ``xgboost``, etc. are a must have. \n",
        "\n",
        "Similarly, the NLP domain has its set of favorites. The following are some of the popular ones:\n",
        "- ``nltk`` : is a leading platform for building NLP applications. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing utilities.\n",
        "- ``gensim`` : is a library for unsupervised topic modeling, document indexing, retrieval by similarity, and other NLP functionalities, using modern statistical machine learning.\n",
        "- ``spacy`` : is a library which provides \"Industrial-Strength NLP\" capabilities which scale and are blazingly fast\n",
        "- ``fasttext`` : is a library for learning of word embeddings and text classification created by Facebook's AI Research lab.\n",
        "- ``huggingface`` ðŸ¤— : is a community and data science platform that provides tools that enable users to build, train and deploy ML models based on open source (OS) code and technologies."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hvjs76wkK72"
      },
      "source": [
        "## Let's Read Some Shakespeare\n",
        "\n",
        "The __Gutenberg Project__ is an amazing project aimed at providing free access to some of the world's most amazing classical works. This makes it a wonderful source of textual data for NLP practitionars to use and improve their understanding of textual data. Ofcourse you can improve your litrary skills too ðŸ˜ƒ\n",
        "\n",
        "``NLTK`` provides us with a nice interface for the _Gutenberg_ project. Apart from some key utilities, this nice and clean interface enables us to access a number of large textual datasets to play with. For this workshop, we will focus on Shakespeare's __Hamlet__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HLIleBHZh8o7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import gutenberg\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "pd.options.display.max_columns=10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMQdAYMElYkt"
      },
      "outputs": [],
      "source": [
        "# First things first, download the Gutenberg Project files\n",
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jLmqfYlXlibu"
      },
      "outputs": [],
      "source": [
        "# get the text for hamlet\n",
        "hamlet_raw = gutenberg.open('shakespeare-hamlet.txt')\n",
        "hamlet_raw = hamlet_raw.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIBUARNOlwHW"
      },
      "outputs": [],
      "source": [
        "# Let us print some text\n",
        "print(hamlet_raw[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6tB6Z_Gl-N3"
      },
      "source": [
        "## Quick Exploratory Analysis\n",
        "\n",
        "Just like any other data science problem, the first step is to understand the dataset itself. NLP is no different."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUL7iz4im_ys"
      },
      "outputs": [],
      "source": [
        "# View a Few raw lines of text\n",
        "\n",
        "# Add your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "28MfrMU-nJBG"
      },
      "outputs": [],
      "source": [
        "# Total Number of lines of text in Hamlet\n",
        "print(\"Total lines in the book/corpus={}\".format(len(hamlet_raw)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NG6WdhpNnXL9"
      },
      "outputs": [],
      "source": [
        "# Total Number of lines of text excluding blank lines\n",
        "hamlet_no_blanks = list(filter(None, [item.strip('\\n') \n",
        "                               for item in hamlet_raw]))\n",
        "hamlet_no_blanks[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ2eFYw8ntff"
      },
      "outputs": [],
      "source": [
        "# Total Number of non-blank lines of text in Hamlet\n",
        "\n",
        "# Add your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tY_3HQe9wiuB"
      },
      "source": [
        "### How Long are the sentences?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUeWqz3QxFAZ"
      },
      "outputs": [],
      "source": [
        "line_lengths = [len(sentence) for sentence in hamlet_no_blanks]\n",
        "p = sns.kdeplot(line_lengths, shade=True, color='yellow')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AE1-ElrPxOAd"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Splitting sentences into usable terms/words is an important aspect of preprocessing textual data. Tokenization is thus the process of identifying the right word boundaries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAKYdZPbxg_r"
      },
      "outputs": [],
      "source": [
        "# simple tokenizer\n",
        "# splitting each sentence to get words\n",
        "tokens = [item.split() for item in hamlet_no_blanks]\n",
        "print(tokens[:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbb8xbQKxp_7"
      },
      "outputs": [],
      "source": [
        "# Let us visualize the distribution of tokens per sentence\n",
        "\n",
        "## Add your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlwlj9RuyJgw"
      },
      "source": [
        "## A bit more clean-up\n",
        "There can be a number of clean-up steps depending upon the kind of dataset and the problem we are solving. \n",
        "\n",
        "In this case, let us cleanup/remove terms which contain any kind of special characters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cjH5YWmiybTB"
      },
      "outputs": [],
      "source": [
        "# only keeping words and removing special characters\n",
        "words = list(filter(None, [re.sub(r'[^A-Za-z]', '', word) for word in words]))\n",
        "print(words[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOPPlL__yh1U"
      },
      "source": [
        "## Can you identify Top Occurring words?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwVkovvCymBn"
      },
      "outputs": [],
      "source": [
        "# Add your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nm6CS9KytxS"
      },
      "source": [
        "### Stopword Removal\n",
        "As you can see from the above output, the top occuring terms are not of much use in terms of understanding the context, etc. In the NLP space, such terms (punctuation marks, prepositions, etc) are termed as stopwords and are typically removed to handle dimensionality and other issues.\n",
        "\n",
        "Thankfully, ``nltk`` provides a clean utility along with an extensible list of stopwords that we can use straight-away"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fo8Te_WfyqqR"
      },
      "outputs": [],
      "source": [
        "import nltk \n",
        "\n",
        "# print a few stop words\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "stopwords[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0UHXmtI4zfR1"
      },
      "outputs": [],
      "source": [
        "# Remove stopwords\n",
        "words = [word.lower() for word in words if word not in stopwords]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJQozwulzlbh"
      },
      "outputs": [],
      "source": [
        "# Top Words by occurance after stopword removal\n",
        "\n",
        "# Add your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gslVjNc7zuWC"
      },
      "source": [
        "## Text Preprocessing\n",
        "\n",
        "We covered some basics of pre-processing so far, steps such as:\n",
        "- Lower-casing\n",
        "- Special character removal\n",
        "- Stopword removal\n",
        "- Removing blank lines and empty spaces\n",
        "\n",
        "are typically performed time and again. There are a number of other steps as well but those are mostly application dependent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l04SS_0k0J5_"
      },
      "outputs": [],
      "source": [
        "# A utility function to perform basic cleanup\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "isbZGhC9BHtO"
      },
      "outputs": [],
      "source": [
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "norm_corpus = normalize_corpus(hamlet_raw)\n",
        "norm_corpus"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "01_getting_started_answers.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}