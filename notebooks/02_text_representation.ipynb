{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5qqNXxRKU1B8"
      },
      "source": [
        "# Text Representation Techniques\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/raghavbali/workshop_text_classification/blob/main/notebooks/02_text_representation.ipynb)\n",
        "\n",
        "In this notebook, we will get familiar with some basic Text Representation Techniques \n",
        "Key takeaways from this notebook are:\n",
        "\n",
        "- Learn how to transform text into usable format using Bag of Words techniques such as:\n",
        "  - Count Vectorizer\n",
        "  - TF-IDF\n",
        "  - Similarity Features\n",
        "\n",
        "![text_repr.png](../assets/text_repr.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jxkAwOUwRrF7"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nltk.corpus import gutenberg\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "pd.options.display.max_columns=10000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IVSjSzvxY4tM"
      },
      "outputs": [],
      "source": [
        "# First things first, download the Gutenberg Project files\n",
        "nltk.download('gutenberg')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5egExbqY6q4"
      },
      "outputs": [],
      "source": [
        "# get the text for hamlet\n",
        "hamlet_raw = gutenberg.open('shakespeare-hamlet.txt')\n",
        "hamlet_raw = hamlet_raw.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g50oL62aY6l5"
      },
      "outputs": [],
      "source": [
        "# A utility function to perform basic cleanup\n",
        "def normalize_document(doc):\n",
        "    # lower case and remove special characters\\whitespaces\n",
        "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
        "    doc = doc.lower()\n",
        "    doc = doc.strip()\n",
        "    # tokenize document\n",
        "    tokens = nltk.word_tokenize(doc)\n",
        "    # filter stopwords out of document\n",
        "    filtered_tokens = [token for token in tokens if token not in stopwords]\n",
        "    # re-create document from filtered tokens\n",
        "    doc = ' '.join(filtered_tokens)\n",
        "    return doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4-Kmr0KZUAl"
      },
      "outputs": [],
      "source": [
        "normalize_corpus = np.vectorize(normalize_document)\n",
        "\n",
        "norm_corpus = normalize_corpus(hamlet_raw)\n",
        "norm_corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Irxq6wRwZJc1"
      },
      "source": [
        "## Bag of Words : Term Frequency\n",
        "A simple vector space representational model for text data. A vector space model is simply a mathematical model for transforming text as numeric vectors, such that each dimension of the vector is a specific feature\\attribute. The bag of words model represents each text document as a numeric vector where each dimension(column) is a specific word from the vocabulary and the value could be its frequency in the document. The model’s name is such because each document is represented literally as a ‘bag’ of its own words, disregarding word orders, sequences and grammar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kn4N1UgpZLUz"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6LktkOWdZNlq"
      },
      "outputs": [],
      "source": [
        "cv = CountVectorizer(min_df=0., max_df=1.)\n",
        "cv_matrix = cv.fit_transform(norm_corpus)\n",
        "cv_matrix = cv_matrix.toarray()\n",
        "cv_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPVvuidsZNjD"
      },
      "outputs": [],
      "source": [
        "cv_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZDQ8_zkZNgG"
      },
      "outputs": [],
      "source": [
        "vocab = cv.get_feature_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wd68S8B3Zbhw"
      },
      "outputs": [],
      "source": [
        "# show document feature vectors\n",
        "pd.DataFrame(cv_matrix, columns=vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbE9z4Moa8dy"
      },
      "source": [
        "## TF-IDF\n",
        "Using absolute frequency counts as a measure of importance has its shortcomings. One potential issue is that there might be some terms which occur frequently across all documents and these may tend to overshadow other terms in the feature set. The TF-IDF model tries to combat this issue by using a normalizing factor. TF-IDF or Term Frequency-Inverse Document Frequency, uses a combination of two metrics in its computation, namely: __term frequency (tf)__ and __inverse document frequency (idf)__.\n",
        "\n",
        "Mathematically, we can define TF-IDF as\n",
        "\n",
        "``TF-IDF = tf x idf``\n",
        "\n",
        "Where, each element in the TF-IDF matrix is the score for word w in document D.\n",
        "\n",
        "The term **tf(w, D)** represents the term frequency of the word **w** in document **D**, which can be obtained from the Bag of Words model.\n",
        "The term idf(w, D) is the inverse document frequency for the term w, which can be computed as the log transform of the total number of documents in the corpus C divided by the document frequency of the word w, in other words it is the frequency of documents in the corpus where the word w occurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8rBwfbPubTv5"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vs9uk2o8bZwh"
      },
      "outputs": [],
      "source": [
        "tv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True)\n",
        "tv_matrix = tv.fit_transform(norm_corpus)\n",
        "tv_matrix = tv_matrix.toarray()\n",
        "tv_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFHSglOybZnX"
      },
      "outputs": [],
      "source": [
        "vocab = tv.get_feature_names()\n",
        "pd.DataFrame(np.round(tv_matrix, 2), columns=vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6L2dbC3acLSt"
      },
      "source": [
        "## Bag of N-Grams Model\n",
        "A word is just a single token, often known as a **unigram** or 1-gram. We already know that the Bag of Words model doesn’t consider order of words. But what if we also wanted to take into account phrases or collection of words which occur in a sequence? **N-grams** help us achieve that. An N-gram is basically a collection of word tokens from a text document such that these tokens are contiguous and occur in a sequence. Bi-grams indicate n-grams of order 2 (two words), Tri-grams indicate n-grams of order 3 (three words), and so on. The Bag of N-Grams model is hence just an extension of the Bag of Words model so we can also leverage N-gram based features. The following example depicts bi-gram based features in each document feature vector."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UkAd3s4LcThI"
      },
      "outputs": [],
      "source": [
        "# you can set the n-gram range to 1,2 to get unigrams as well as bigrams\n",
        "bv = TfidfVectorizer(min_df=0., max_df=1., use_idf=True,ngram_range=(2,2))\n",
        "bv_matrix = bv.fit_transform(norm_corpus)\n",
        "\n",
        "bv_matrix = bv_matrix.toarray()\n",
        "bv_matrix.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGGohxNycgrG"
      },
      "outputs": [],
      "source": [
        "vocab = bv.get_feature_names()\n",
        "pd.DataFrame(bv_matrix, columns=vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikWNCAPHcqIU"
      },
      "source": [
        "## Similarity Based Features\n",
        "Now that we have a method to transform text into vector form, we can now build on top of such features we engineered to generate new features which can be useful in domains like search engines, document clustering and information retrieval by leveraging these similarity based features.\n",
        "\n",
        "Pairwise document/sentence/term similarity in a corpus involves computing  similarity for each pair of entities in a corpus. Thus if we have N entities in a corpus, we would end up with a N x N matrix such that each row and column represents the similarity score for a given pair. \n",
        "\n",
        "There are several similarity and distance metrics that are used to compute  similarity. These include :\n",
        "- cosine distance/similarity, \n",
        "- euclidean distance, \n",
        "- manhattan distance, \n",
        "- BM25 similarity, \n",
        "- jaccard distance and so on. \n",
        "\n",
        "```shell\n",
        "Add image from the slide deck\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmpC-Z0sdt_I"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMjIhdzhdt8D"
      },
      "outputs": [],
      "source": [
        "similarity_matrix = cosine_similarity(tv_matrix)\n",
        "similarity_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2qwGs6ndt37"
      },
      "outputs": [],
      "source": [
        "similarity_df = pd.DataFrame(similarity_matrix)\n",
        "similarity_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "02_text_representation.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
